#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬
åŒ…å«æƒé‡ä¿å­˜ç­–ç•¥ã€æŸå¤±å‡½æ•°åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
"""

import os
import sys
import argparse
import time
import random
import logging
import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch.nn.functional as F
import gc
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Rectangle

from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, ToTensor, Normalize, ColorJitter, RandomAffine
from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, ReduceLROnPlateau

from dataset.data_loader import RSDataset
from model.loss import yolo_loss, build_target, adjust_learning_rate
from model.loss import build_target_anchorfree, anchorfree_loss
from utils.utils import AverageMeter, eval_iou_acc
from utils.checkpoint import save_checkpoint, load_pretrain
from model.swin_moe_geo_config import swin_moe_geo_cfg
# from visualization_core import draw_visualization  # æš‚æ—¶æ³¨é‡Šï¼Œå…ˆè®©è®­ç»ƒè·‘èµ·æ¥
from model.swin_moe_geo import SwinTransformer_MoE_MultiInput
from model.anchorfree_head import AnchorFreeHead

def setup_multi_gpu(args):
    """
    è®¾ç½®å¤šGPUè®­ç»ƒ
    @param args: å‚æ•°å¯¹è±¡
    @return: device_ids, device
    """
    if args.gpu:
        # è§£æGPUè®¾å¤‡åˆ—è¡¨
        if ',' in args.gpu:
            device_ids = [int(id.strip()) for id in args.gpu.split(',')]
        else:
            device_ids = [int(args.gpu)]
        
        # è®¾ç½®ä¸»è®¾å¤‡
        device = torch.device(f'cuda:{device_ids[0]}')
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        available_gpus = []
        for gpu_id in device_ids:
            if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
                available_gpus.append(gpu_id)
            else:
                logging.warning(f"GPU {gpu_id} ä¸å¯ç”¨ï¼Œè·³è¿‡")
        
        if not available_gpus:
            logging.error("æ²¡æœ‰å¯ç”¨çš„GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")
            device = torch.device('cpu')
            device_ids = []
        else:
            device_ids = available_gpus
            logging.info(f"ä½¿ç”¨GPUè®¾å¤‡: {device_ids}")
            
        return device_ids, device
    else:
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        if torch.cuda.is_available():
            device_ids = list(range(torch.cuda.device_count()))
            device = torch.device('cuda:0')
            logging.info(f"ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU: {device_ids}")
        else:
            device_ids = []
            device = torch.device('cpu')
            logging.info("ä½¿ç”¨CPUè®­ç»ƒ")
        
        return device_ids, device

def wrap_model_for_multi_gpu(model, device_ids, args):
    """
    å°†æ¨¡å‹åŒ…è£…ä¸ºå¤šGPUè®­ç»ƒ
    @param model: æ¨¡å‹
    @param device_ids: GPUè®¾å¤‡IDåˆ—è¡¨
    @param args: å‚æ•°å¯¹è±¡
    @return: åŒ…è£…åçš„æ¨¡å‹
    """
    if len(device_ids) > 1:
        # å¤šGPUè®­ç»ƒ - å…ˆå°†æ¨¡å‹ç§»åŠ¨åˆ°ä¸»è®¾å¤‡
        main_device = f'cuda:{device_ids[0]}'
        model = model.to(main_device)
        
        # ç¡®ä¿BatchNormä½¿ç”¨åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        for module in model.modules():
            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                module.track_running_stats = True
                module.momentum = 0.1  # ä½¿ç”¨è¾ƒå°çš„momentumï¼Œæé«˜ç¨³å®šæ€§
        
        if args.distributed:
            # åˆ†å¸ƒå¼è®­ç»ƒ
            model = nn.parallel.DistributedDataParallel(model, device_ids=device_ids)
            logging.info(f"ä½¿ç”¨DistributedDataParallelï¼Œè®¾å¤‡: {device_ids}")
        else:
            # DataParallelè®­ç»ƒ - æ·»åŠ åŒæ­¥BN
            model = nn.DataParallel(model, device_ids=device_ids)
            logging.info(f"ä½¿ç”¨DataParallelï¼Œè®¾å¤‡: {device_ids}")
    else:
        # å•GPUè®­ç»ƒ
        if device_ids:
            model = model.to(f'cuda:{device_ids[0]}')
            logging.info(f"ä½¿ç”¨å•GPUè®­ç»ƒï¼Œè®¾å¤‡: {device_ids[0]}")
        else:
            model = model.to('cpu')
            logging.info("ä½¿ç”¨CPUè®­ç»ƒ")
    
    return model

def custom_collate_fn(batch):
    queryimg_4ch, rsimg, bbox, idx, click_xy, ori_hw = zip(*batch)
    queryimg_4ch = torch.stack(queryimg_4ch)
    rsimg = torch.stack(rsimg)
    # å…³é”®ä¿®æ­£ï¼šä¿è¯æ¯ä¸ªbboxéƒ½æ˜¯1ç»´4å…ƒç´ tensor
    bbox = [b.view(-1) if isinstance(b, torch.Tensor) else torch.tensor(b, dtype=torch.float32).view(-1) for b in bbox]
    bbox = torch.stack(bbox)
    idx = torch.tensor(idx)
    click_xy = torch.stack(click_xy)
    ori_hw = torch.stack(ori_hw)
    return queryimg_4ch, rsimg, bbox, idx, click_xy, ori_hw

class EnhancedTrainer:
    def __init__(self, args):
        self.args = args
        self.best_accu = -float('Inf')
        self.worst_accu = float('Inf')
        self.best_epoch = 0
        self.worst_epoch = 0
        
        # åˆ›å»ºæƒé‡ä¿å­˜ç›®å½•
        self.weight_dir = f'./saved_weights/{args.savename}'
        os.makedirs(self.weight_dir, exist_ok=True)
        
        # æŸå¤±å†å²è®°å½•
        self.loss_history = {
            'total_loss': [],
            'heatmap_loss': [],
            'bbox_loss': [],
            'accu50': [],
            'accu25': [],
            'mean_iou': []
        }
        
    def save_weights(self, model, optimizer, scheduler, epoch, accu, is_best=False, is_worst=False):
        """åªä¿å­˜æœ€ä½³ã€æœ€å·®å’Œæœ€ç»ˆæƒé‡ï¼Œå¹¶ä¿å­˜swin_cfgç”¨äºåç»­å¯¹æ¯”"""
        from model.swin_moe_geo_config import swin_moe_geo_cfg  # ç¡®ä¿æ¯æ¬¡ä¿å­˜éƒ½æ˜¯æœ€æ–°çš„
        checkpoint = {
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'accu': accu,
            'optimizer': optimizer.state_dict(),
            'scheduler': scheduler.state_dict() if scheduler else None,
            'swin_cfg': dict(swin_moe_geo_cfg),  # æ–°å¢ï¼šä¿å­˜å½“å‰config
        }
        # æœ€ç»ˆæƒé‡ï¼ˆæ¯è½®è¦†ç›–ï¼‰
        torch.save(checkpoint, os.path.join(self.weight_dir, 'final_weights.pth'))
        # æœ€ä½³æƒé‡
        if is_best:
            torch.save(checkpoint, os.path.join(self.weight_dir, 'best_weights.pth'))
            self.best_accu = accu
            self.best_epoch = epoch
            logging.info("[OK] æ–°çš„æœ€ä½³æƒé‡ (Epoch {epoch+1}, Accu: {accu:.4f})")
        # æœ€å·®æƒé‡
        if is_worst:
            torch.save(checkpoint, os.path.join(self.weight_dir, 'worst_weights.pth'))
            self.worst_accu = accu
            self.worst_epoch = epoch
            logging.warning("[WARNING] æ–°çš„æœ€å·®æƒé‡ (Epoch {epoch+1}, Accu: {accu:.4f})")
        logging.info(f"æƒé‡å·²ä¿å­˜ - Epoch {epoch+1}, Accu: {accu:.4f}")
    
    def analyze_loss_function(self, heatmap_loss, bbox_loss, total_loss, accu50, accu25, mean_iou):
        """åˆ†ææŸå¤±å‡½æ•°æ•ˆæœ"""
        # ä¿è¯åªä¿å­˜floatæ•°å€¼
        self.loss_history['total_loss'].append(float(total_loss))
        self.loss_history['heatmap_loss'].append(float(heatmap_loss))
        self.loss_history['bbox_loss'].append(float(bbox_loss))
        self.loss_history['accu50'].append(float(accu50))
        self.loss_history['accu25'].append(float(accu25))
        self.loss_history['mean_iou'].append(float(mean_iou))
        
        # è®¡ç®—æŸå¤±æ¯”ä¾‹
        loss_ratio = heatmap_loss / bbox_loss if bbox_loss > 0 else float('inf')
        
        # åˆ†ææŸå¤±å‡½æ•°åˆç†æ€§
        analysis = {
            'loss_ratio': loss_ratio,
            'is_balanced': 0.1 < loss_ratio < 10.0,
            'heatmap_dominating': loss_ratio > 10.0,
            'bbox_dominating': loss_ratio < 0.1,
            'loss_decreasing': len(self.loss_history['total_loss']) > 1 and 
                              self.loss_history['total_loss'][-1] < self.loss_history['total_loss'][-2],
            'accu_improving': len(self.loss_history['accu50']) > 1 and 
                             self.loss_history['accu50'][-1] > self.loss_history['accu50'][-2]
        }
        
        return analysis
    
    def visualize_loss_analysis(self, save_path='loss_analysis.png'):
        """å¯è§†åŒ–æŸå¤±å‡½æ•°åˆ†æ"""
        if len(self.loss_history['total_loss']) < 2:
            logging.info("éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®æ‰èƒ½è¿›è¡ŒæŸå¤±åˆ†æ")
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.loss_history['total_loss']) + 1)
        
        # æŸå¤±å˜åŒ–è¶‹åŠ¿
        axes[0, 0].plot(epochs, self.loss_history['total_loss'], 'b-', label='Total Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.loss_history['heatmap_loss'], 'r-', label='Heatmap Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.loss_history['bbox_loss'], 'g-', label='BBox Loss', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss Value')
        axes[0, 0].set_title('Loss Components Over Training')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # æŸå¤±æ¯”ä¾‹
        loss_ratios = [h/b if b > 0 else float('inf') for h, b in zip(self.loss_history['heatmap_loss'], self.loss_history['bbox_loss'])]
        axes[0, 1].plot(epochs, loss_ratios, 'purple', linewidth=2)
        axes[0, 1].axhline(y=1, color='black', linestyle='--', alpha=0.5)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Heatmap/BBox Loss Ratio')
        axes[0, 1].set_title('Loss Balance Over Training')
        axes[0, 1].grid(True)
        
        # å‡†ç¡®ç‡å˜åŒ–
        axes[1, 0].plot(epochs, self.loss_history['accu50'], 'orange', linewidth=2, label='Accu50')
        axes[1, 0].plot(epochs, self.loss_history['accu25'], 'cyan', linewidth=2, label='Accu25')
        axes[1, 0].plot(epochs, self.loss_history['mean_iou'], 'green', linewidth=2, label='Mean IoU')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].set_title('Model Performance Over Training')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # æŸå¤±ä¸æ€§èƒ½å…³ç³»
        axes[1, 1].scatter(self.loss_history['total_loss'], self.loss_history['accu50'], alpha=0.7)
        axes[1, 1].set_xlabel('Total Loss')
        axes[1, 1].set_ylabel('Accu50')
        axes[1, 1].set_title('Loss vs Performance')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        logging.info(f"æŸå¤±åˆ†æå›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    def visualize_model_outputs(self, model, data_loader, num_samples=5, save_dir='./visualization_outputs'):
        """åªé’ˆå¯¹swinmoeåˆ†æ”¯çš„å¯è§†åŒ–ï¼Œæ‰€æœ‰ç‚¹å’Œæ¡†ä¸¥æ ¼åšæ˜ å°„ï¼Œæ‰€æœ‰å­å›¾å°ºå¯¸ä¸€è‡´"""
        os.makedirs(save_dir, exist_ok=True)
        model.eval()
        sample_count = 0
        with torch.no_grad():
            for query_imgs, rs_imgs, mat_clickxy, ori_gt_bbox, _, click_xy, ori_img_shape in data_loader:
                if sample_count >= num_samples:
                    break
                # ç»Ÿä¸€ä½¿ç”¨ä¸»è®¾å¤‡
                query_imgs = query_imgs.to(device)
                rs_imgs = rs_imgs.to(device)
                ori_gt_bbox = ori_gt_bbox.to(device)
                mat_clickxy = mat_clickxy.to(device) if mat_clickxy is not None else None
                B, _, H, W = query_imgs.shape
                for i in range(B):
                    if sample_count >= num_samples:
                        break
                    qimg = self.denormalize_image(query_imgs[i])
                    simg = self.denormalize_image(rs_imgs[i])
                    
                    # ä¿®æ­£ï¼šç¡®ä¿clickxyæ˜¯å•ä¸ªç‚¹å‡»ç‚¹åæ ‡å¯¹
                    if mat_clickxy is not None:
                        clickxy = mat_clickxy[i].cpu().numpy()
                        # å¦‚æœclickxyæ˜¯æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–è½¬æ¢ä¸ºåæ ‡å¯¹
                        if clickxy.ndim > 1:
                            clickxy = clickxy.flatten()[:2]  # åªå–å‰ä¸¤ä¸ªå…ƒç´ ä½œä¸º(x,y)
                        elif len(clickxy) > 2:
                            clickxy = clickxy[:2]  # åªå–å‰ä¸¤ä¸ªå…ƒç´ 
                        clickxy = tuple(clickxy)  # è½¬æ¢ä¸ºåæ ‡å¯¹
                    else:
                        clickxy = None
                    
                    # æ–°å¢ï¼šè·å–åŸå§‹ç‚¹å‡»ç‚¹
                    raw_click_xy = click_xy[i].cpu().numpy() if hasattr(click_xy[i], 'cpu') else click_xy[i]
                    # åˆ¤æ–­æ˜¯å¦å½’ä¸€åŒ–
                    if raw_click_xy.max() > 1.5:
                        click_xy_norm = (raw_click_xy[0] / W, raw_click_xy[1] / H)
                    else:
                        click_xy_norm = raw_click_xy
                    clickxy_pixel = (click_xy_norm[0] * W, click_xy_norm[1] * H)
                    
                    heatmap_pred, bbox_pred = model(query_imgs[i:i+1], rs_imgs[i:i+1])
                    _, _, hH, hW = heatmap_pred.shape
                    from model.loss import build_target_anchorfree
                    gt_heatmap, gt_bbox, mask = build_target_anchorfree(
                        ori_gt_bbox[i:i+1], hH, hW, self.args.img_size, self.args.img_size)
                    pred_hm = heatmap_pred[0, 0].sigmoid().cpu().numpy()
                    gt_hm = gt_heatmap[0, 0].cpu().numpy()
                    pred_center = np.unravel_index(pred_hm.argmax(), pred_hm.shape)
                    gt_center = np.unravel_index(gt_hm.argmax(), gt_hm.shape)
                    pred_box_params = bbox_pred[0, :, pred_center[0], pred_center[1]].cpu().numpy()
                    gt_box_params = gt_bbox[0, :, gt_center[0], gt_center[1]].cpu().numpy()
                    bbox_pred_values = bbox_pred[0].cpu().numpy().flatten()
                    
                    # æ–°å¢ï¼šæ”¯æŒåŸå§‹å›¾ç‰‡å°ºå¯¸
                    if len(ori_img_shape) >= 7:
                        ori_img_shape = ori_img_shape[6][i] if isinstance(ori_img_shape[6], (list, tuple)) else ori_img_shape[6]
                    else:
                        ori_img_shape = simg.shape[:2]  # å…œåº•
                    ori_H, ori_W = ori_img_shape
                    img_H, img_W = simg.shape[:2]

                    # çœŸå®æ¡†å’Œä¸­å¿ƒç‚¹ç¼©æ”¾åˆ°å¯è§†åŒ–å°ºå¯¸
                    gt_box_pixel = ori_gt_bbox[i].cpu().numpy() if hasattr(ori_gt_bbox[i], 'cpu') else ori_gt_bbox[i]
                    x1, y1, x2, y2 = gt_box_pixel
                    scale_x = img_W / ori_W
                    scale_y = img_H / ori_H
                    x1 = x1 * scale_x
                    x2 = x2 * scale_x
                    y1 = y1 * scale_y
                    y2 = y2 * scale_y
                    gt_box_pixel = [x1, y1, x2, y2]
                    gt_center_pixel = ((x1 + x2) / 2, (y1 + y2) / 2)

                    # è·å–ç‰¹å¾å›¾å°ºå¯¸ï¼ˆç”¨äºanchor-freeè§£ç å’Œè¿‡æ»¤ï¼‰
                    feat_H, feat_W = pred_hm.shape

                    # 2. é¢„æµ‹æ¡†å’Œä¸­å¿ƒç‚¹ anchor-free è§£ç 
                    def box_params_to_pixel(box_params, center, img_W, img_H, feat_W, feat_H):
                        ny, nx = center  # (y, x)
                        cx_nx, cy_ny, w, h = box_params
                        w = np.clip(w, 1e-2, feat_W)
                        h = np.clip(h, 1e-2, feat_H)
                        cx = nx + cx_nx
                        cy = ny + cy_ny
                        scale_x = img_W / feat_W
                        scale_y = img_H / feat_H
                        cx_img = (cx + 0.5) * scale_x
                        cy_img = (cy + 0.5) * scale_y
                        w_img = w * scale_x
                        h_img = h * scale_y
                        x1 = cx_img - w_img / 2
                        y1 = cy_img - h_img / 2
                        x2 = cx_img + w_img / 2
                        y2 = cy_img + h_img / 2
                        x1 = np.clip(x1, 0, img_W)
                        y1 = np.clip(y1, 0, img_H)
                        x2 = np.clip(x2, 0, img_W)
                        y2 = np.clip(y2, 0, img_H)
                        return [x1, y1, x2, y2]
                    pred_score = pred_hm[pred_center[0], pred_center[1]] if pred_hm is not None else 0
                    if pred_box_params is not None and pred_score > 0.3 and np.all(np.abs(pred_box_params) < feat_W*2):
                        pred_box_pixel = box_params_to_pixel(pred_box_params, pred_center, img_W, img_H, feat_W, feat_H)
                    else:
                        pred_box_pixel = None
                    def center_feat2pixel(center, img_H, img_W, feat_H, feat_W):
                        y, x = center
                        x_pixel = (x + 0.5) * img_W / feat_W
                        y_pixel = (y + 0.5) * img_H / feat_H
                        return (x_pixel, y_pixel)
                    pred_center_pixel = center_feat2pixel(pred_center, img_H, img_W, feat_H, feat_W) if pred_center is not None else None

                    # æŸ¥è¯¢å›¾åƒç‚¹å‡»ç‚¹ç¼©æ”¾åˆ°å¯è§†åŒ–å°ºå¯¸
                    ori_query_H, ori_query_W = ori_img_shape
                    img_H, img_W = qimg.shape[:2]
                    # ä¿®æ­£ï¼šå®‰å…¨åœ°è·å–ç‚¹å‡»ç‚¹åæ ‡ï¼Œæ”¯æŒå°åŒºåŸŸç‚¹å‡»
                    if isinstance(click_xy, (list, tuple, np.ndarray)):
                        click_data = click_xy[i]
                        if isinstance(click_data, (list, tuple, np.ndarray)):
                            click_data = np.asarray(click_data).flatten()
                            if len(click_data) >= 2:
                                click_x, click_y = click_data[0], click_data[1]  # å–å‰ä¸¤ä¸ªå…ƒç´ ä½œä¸ºä¸­å¿ƒç‚¹
                            elif len(click_data) == 1:
                                click_x, click_y = click_data[0], click_data[0]  # å•ä¸ªå€¼å¤åˆ¶
                            else:
                                click_x, click_y = 0, 0  # å…œåº•å€¼
                        else:
                            click_x, click_y = click_data, click_data  # å•ä¸ªå€¼
                    else:
                        click_x, click_y = click_xy, click_xy  # å…œåº•
                    scale_x = img_W / ori_query_W
                    scale_y = img_H / ori_query_H
                    clickxy_pixel = (click_x * scale_x, click_y * scale_y)

                    # æ ¼å¼æ£€æŸ¥å’Œè½¬æ¢ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯æ­£ç¡®çš„æ ¼å¼
                    def ensure_coord_format(coord, name):
                        """ç¡®ä¿åæ ‡æ˜¯æ ‡é‡æˆ–åæ ‡å¯¹æ ¼å¼"""
                        if coord is None:
                            return None
                        if isinstance(coord, (list, tuple, np.ndarray)):
                            coord = np.asarray(coord).flatten()
                            if len(coord) >= 2:
                                return tuple(coord[:2])  # åªå–å‰ä¸¤ä¸ªå…ƒç´ 
                            elif len(coord) == 1:
                                return (coord[0], coord[0])  # å•ä¸ªå€¼å¤åˆ¶ä¸ºåæ ‡å¯¹
                            else:
                                return None
                        else:
                            return (coord, coord)  # æ ‡é‡è½¬æ¢ä¸ºåæ ‡å¯¹
                    
                    # ç¡®ä¿æ‰€æœ‰åæ ‡éƒ½æ˜¯æ­£ç¡®æ ¼å¼
                    clickxy_pixel = ensure_coord_format(clickxy_pixel, 'clickxy_pixel')
                    pred_center_pixel = ensure_coord_format(pred_center_pixel, 'pred_center_pixel')
                    gt_center_pixel = ensure_coord_format(gt_center_pixel, 'gt_center_pixel')
                    
                    # ç¡®ä¿æ¡†æ˜¯åˆ—è¡¨æ ¼å¼
                    if pred_box_pixel is not None and not isinstance(pred_box_pixel, list):
                        pred_box_pixel = list(pred_box_pixel)
                    if gt_box_pixel is not None and not isinstance(gt_box_pixel, list):
                        gt_box_pixel = list(gt_box_pixel)

                    # 4. ä¼ å…¥draw_visualization
                    # draw_visualization(  # æš‚æ—¶æ³¨é‡Šï¼Œå…ˆè®©è®­ç»ƒè·‘èµ·æ¥
                    #     qimg, simg, clickxy_pixel,
                    #     pred_hm, gt_hm,
                    #     pred_box_pixel, gt_box_pixel,
                    #     pred_center_pixel, gt_center_pixel,
                    #     bbox_pred_values,
                    #     save_dir, sample_count+1,
                    #     (img_W, img_H), (feat_H, feat_W)
                    # )
                    sample_count += 1
        logging.info(f"æ‰€æœ‰å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")

    def denormalize_image(self, img_tensor):
        """
        å°†å½’ä¸€åŒ–çš„å›¾åƒå¼ é‡è½¬æ¢å›åŸå§‹åƒç´ å€¼
        @param img_tensor: [C, H, W] å½’ä¸€åŒ–çš„å›¾åƒå¼ é‡
        @return: [H, W, C] numpyæ•°ç»„ï¼Œåƒç´ å€¼èŒƒå›´[0, 255]
        """
        # ImageNetå½’ä¸€åŒ–å‚æ•°
        device = img_tensor.device
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)
        
        # åå½’ä¸€åŒ–
        img_tensor = img_tensor * std + mean
        
        # è½¬æ¢åˆ°[0, 1]èŒƒå›´
        img_tensor = torch.clamp(img_tensor, 0, 1)
        
        # è½¬æ¢åˆ°[0, 255]èŒƒå›´å¹¶è½¬ä¸ºnumpy
        img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
        
        # è½¬æ¢é€šé“é¡ºåº [C, H, W] -> [H, W, C]
        img_np = np.transpose(img_np, (1, 2, 0))
        
        return img_np

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆè·¨è§†è§’ç›®æ ‡å®šä½è®­ç»ƒ')
    parser.add_argument('--max_epoch', default=25, type=int, help='training epoch')
    parser.add_argument('--lr', default=1.0e-4, type=float, help='learning rate')  # åŸºäºæœ€ä½³æ—¥å¿—çš„ä¼˜åŒ–å­¦ä¹ ç‡
    parser.add_argument('--batch_size', default=8, type=int, help='batch size')  # ä¿æŒæœ€ä½³æ‰¹æ¬¡å¤§å°
    parser.add_argument('--img_size', default=1024, type=int, help='image size')
    parser.add_argument('--data_root', default='data', type=str, help='data root')
    parser.add_argument('--data_name', default='CVOGL_DroneAerial', type=str, help='data name')
    parser.add_argument('--gpu', default='0', type=str, help='gpu id')
    parser.add_argument('--num_workers', default=24, type=int, help='num workers')
    parser.add_argument('--savename', default='optimized_enhanced_25epoch', type=str, help='save name')
    parser.add_argument('--print_freq', default=50, type=int, help='print frequency')
    parser.add_argument('--seed', default=42, type=int, help='random seed')
    parser.add_argument('--beta', default=1.0, type=float, help='the weight of cls loss')  # æ¢å¤åŸå§‹æƒé‡
    parser.add_argument('--model', default='swinmoe', type=str, help='model name')
    parser.add_argument('--cosine', action='store_true', default=True, help='use cosine annealing')
    parser.add_argument('--weight_decay', default=1e-4, type=float, help='weight decay')
    parser.add_argument('--lambda-entropy-base', default=0.005, type=float, help='entropy regularization base')  # æ¢å¤åŸæ¥çš„ç†µæ­£åˆ™åŒ–æƒé‡
    parser.add_argument('--test', action='store_true', help='test mode')
    parser.add_argument('--val', dest='val', default=False, action='store_true', help='val')
    parser.add_argument('--pretrain', default='', type=str, metavar='PATH')
    parser.add_argument('--visualize', dest='visualize', default=False, action='store_true', help='visualize results')
    parser.add_argument('--no-moe-entropy', action='store_true', help='If set, do not use MoEé—¨æ§ç†µæ­£åˆ™é¡¹')
    parser.add_argument('--distributed', action='store_true', help='Use distributed training')
    
    args = parser.parse_args()
    
    # è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€æ—¥å¿—æ–‡ä»¶å
    if not os.path.exists('./logs'):
        os.mkdir('./logs')
    now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    log_name = f"{args.savename}_{now_str}.log"
    log_path = os.path.join('logs', log_name)
    # æ¸…ç©ºæ‰€æœ‰å·²å­˜åœ¨çš„handler
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œç»ˆç«¯
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)-15s %(levelname)-8s %(message)s",
        handlers=[
            logging.FileHandler(log_path, mode="a+"),
            logging.StreamHandler()
        ]
    )
    logging.info(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {log_path}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed+1)
    torch.manual_seed(args.seed+2)
    torch.cuda.manual_seed_all(args.seed+3)
    
    # è®¾ç½®å¤šGPUè®­ç»ƒ
    device_ids, device = setup_multi_gpu(args)
    logging.info(f"ä¸»è®¾å¤‡: {device}")
    logging.info(f"å¯ç”¨GPUæ•°é‡: {len(device_ids)}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        logging.info(f"CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logging.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        logging.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # æ ¹æ®GPUæ•°é‡è°ƒæ•´batch_size
    if len(device_ids) > 1:
        original_batch_size = args.batch_size
        args.batch_size = args.batch_size * len(device_ids)
        logging.info(f"å¤šGPUè®­ç»ƒï¼Œbatch_sizeä»{original_batch_size}è°ƒæ•´ä¸º{args.batch_size}")
        
        # åŸºäºå†å²æœ€ä½³ç»“æœä¼˜åŒ–å­¦ä¹ ç‡
        if len(device_ids) > 1:
            # å¤šGPUè®­ç»ƒï¼Œæ ¹æ®batch sizeè°ƒæ•´å­¦ä¹ ç‡
            # ä½¿ç”¨çº¿æ€§ç¼©æ”¾è§„åˆ™ï¼šlr = base_lr * (batch_size / base_batch_size)
            base_batch_size = 8
            scale_factor = args.batch_size / base_batch_size
            args.lr = 1e-4 * scale_factor  # åŸºç¡€å­¦ä¹ ç‡1e-4ï¼Œæ ¹æ®batch sizeç¼©æ”¾
            logging.info(f"å¤šGPUè®­ç»ƒï¼Œå­¦ä¹ ç‡è°ƒæ•´ä¸º: {args.lr} (ç¼©æ”¾å› å­: {scale_factor})")
        elif len(device_ids) == 1:
            # å•GPUè®­ç»ƒï¼Œä½¿ç”¨å†å²æœ€ä½³å­¦ä¹ ç‡
            args.lr = 1e-4
            logging.info(f"å•GPUè®­ç»ƒï¼Œå­¦ä¹ ç‡: {args.lr}")
    else:
        # å•GPUè®­ç»ƒ
        args.lr = 1e-4
        logging.info(f"å•GPUè®­ç»ƒï¼Œå­¦ä¹ ç‡: {args.lr}")
    
    # åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
    trainer = EnhancedTrainer(args)
    
    # æ•°æ®åŠ è½½
    # å¢å¼ºçš„æ•°æ®å˜æ¢ç­–ç•¥
    from torchvision.transforms import functional as F
    
    class CrossViewAugment:
        """è·¨è§†è§’æ•°æ®å¢å¼º"""
        def __init__(self, p=0.7):  # å¢åŠ å¢å¼ºæ¦‚ç‡
            self.p = p
            
        def __call__(self, query_img, sat_img):
            if random.random() < self.p:
                # åŒæ­¥å¢å¼ºï¼šä¿æŒè·¨è§†è§’ä¸€è‡´æ€§
                if random.random() < 0.3:
                    # åŒæ­¥æ—‹è½¬
                    angle = random.uniform(-15, 15)
                    query_img = F.rotate(query_img, angle)
                    sat_img = F.rotate(sat_img, angle)
                
                if random.random() < 0.3:
                    # åŒæ­¥ç¿»è½¬
                    if random.random() < 0.5:
                        query_img = F.hflip(query_img)
                        sat_img = F.hflip(sat_img)
                    else:
                        query_img = F.vflip(query_img)
                        sat_img = F.vflip(sat_img)
                
                if random.random() < 0.3:
                    # é¢œè‰²å¢å¼º
                    brightness = random.uniform(0.8, 1.2)
                    contrast = random.uniform(0.8, 1.2)
                    saturation = random.uniform(0.8, 1.2)
                    hue = random.uniform(-0.1, 0.1)
                    
                    query_img = F.adjust_brightness(query_img, brightness)
                    query_img = F.adjust_contrast(query_img, contrast)
                    query_img = F.adjust_saturation(query_img, saturation)
                    query_img = F.adjust_hue(query_img, hue)
                    
                    sat_img = F.adjust_brightness(sat_img, brightness)
                    sat_img = F.adjust_contrast(sat_img, contrast)
                    sat_img = F.adjust_saturation(sat_img, saturation)
                    sat_img = F.adjust_hue(sat_img, hue)
            
            return query_img, sat_img
    
    # åŸºç¡€å˜æ¢
    base_transform = Compose([
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # å¢å¼ºå˜æ¢
    augment_transform = Compose([
        ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),      # é€‚åº¦å¢å¼ºå¼ºåº¦
        RandomAffine(degrees=10, translate=(0.08, 0.08), scale=(0.9, 1.1)),       # é€‚åº¦å‡ ä½•å˜æ¢
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # ä½¿ç”¨å¢å¼ºå˜æ¢
    input_transform = augment_transform
    
    train_dataset = RSDataset(data_root=args.data_root, data_name=args.data_name,
                             split_name='train', img_size=args.img_size,
                             transform=input_transform, augment=True)
    val_dataset = RSDataset(data_root=args.data_root, data_name=args.data_name,
                           split_name='val', img_size=args.img_size, transform=input_transform)
    
    # Windowsç³»ç»Ÿå¤šè¿›ç¨‹é—®é¢˜ä¿®å¤ï¼šå‡å°‘workeræ•°é‡æˆ–ç¦ç”¨å¤šè¿›ç¨‹
    if os.name == 'nt':  # Windowsç³»ç»Ÿ
        num_workers = min(args.num_workers, 4)  # é™åˆ¶æœ€å¤§workeræ•°é‡
        if num_workers > 0:
            logging.info(f"Windowsç³»ç»Ÿï¼Œè°ƒæ•´workeræ•°é‡: {args.num_workers} -> {num_workers}")
        else:
            num_workers = 0
            logging.info("Windowsç³»ç»Ÿï¼Œç¦ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½")
    else:
        num_workers = args.num_workers
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,
                             pin_memory=True, drop_last=False, num_workers=num_workers,
                             collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,
                             pin_memory=True, drop_last=False, num_workers=num_workers,
                             collate_fn=custom_collate_fn)
    
    # æ¨¡å‹åˆ›å»º
    if args.model == 'swinmoe':
        from model.swin_moe_geo import SwinTransformer_MoE_MultiInput
        from model.anchorfree_head import AnchorFreeHead
        
        swin_cfg = swin_moe_geo_cfg
        
        # åˆ›å»ºè‡ªå®šä¹‰çš„SwinTransformer_MoE_MultiInputï¼Œæ”¯æŒä¸åŒé€šé“æ•°
        class CustomSwinTransformer_MoE_MultiInput(SwinTransformer_MoE_MultiInput):
            def __init__(self, **kwargs):
                super().__init__(**kwargs)
                # é‡æ–°åˆ›å»ºpatch embeddingsï¼Œä¸ºä¸åŒæ•°æ®é›†ä½¿ç”¨ä¸åŒé€šé“æ•°
                from model.swin_moe_geo import PatchEmbed
                self.patch_embeds = nn.ModuleDict({
                    'query': PatchEmbed(in_channels=4, embed_dim=kwargs.get('embed_dim', 96), patch_size=kwargs.get('patch_size', 4)),  # æŸ¥è¯¢å›¾åƒ4é€šé“
                    'sat': PatchEmbed(in_channels=3, embed_dim=kwargs.get('embed_dim', 96), patch_size=kwargs.get('patch_size', 4))      # å«æ˜Ÿå›¾åƒ3é€šé“
                })
            
            def forward(self, query_img, sat_img):
                """é‡è½½forwardæ–¹æ³•ï¼Œæ¥å—query_imgå’Œsat_imgå‚æ•°"""
                # è°ƒç”¨çˆ¶ç±»çš„forwardæ–¹æ³•ï¼Œä¼ é€’æ­£ç¡®çš„å‚æ•°æ ¼å¼
                return super().forward([query_img, sat_img])
        
        swin_backbone = CustomSwinTransformer_MoE_MultiInput(
            in_channels=4,  # è¿™ä¸ªå‚æ•°åœ¨è‡ªå®šä¹‰ç±»ä¸­ä¼šè¢«å¿½ç•¥
            embed_dim=swin_cfg.get('embed_dim', 96),
            patch_size=swin_cfg.get('patch_size', 4),
            window_size=swin_cfg.get('window_size', 7),
            depths=swin_cfg.get('depths', (2,2,6,2)),
            num_heads=swin_cfg.get('num_heads', (3,6,12,24)),
            ffn_ratio=swin_cfg.get('ffn_ratio', 4),
            num_experts=6,  # å›ºå®š6ä¸ªä¸“å®¶
            top_k=2,        # å›ºå®šä½¿ç”¨2ä¸ªä¸“å®¶
            moe_block_indices=swin_cfg.get('moe_block_indices', None),
            datasets=swin_cfg.get('datasets', ('query','sat'))
        )
        
        out_dim = swin_backbone.out_dim
        anchorfree_head = AnchorFreeHead(in_channels=out_dim, feat_channels=256, num_classes=1)
        
        class DetGeoSwinMoE_AF(nn.Module):
            def __init__(self, backbone, head):
                super().__init__()
                self.backbone = backbone
                self.head = head
                self.moe_entropy = 0.0  # ç¡®ä¿åˆå§‹åŒ–
                
            def forward(self, query_img, sat_img, click_map=None):
                # å‰å‘ä¼ æ’­
                backbone_output = self.backbone(query_img, sat_img)
                if isinstance(backbone_output, tuple):
                    query_vec, sat_feat, avg_entropy = backbone_output
                    # ç¡®ä¿ç†µå€¼åœ¨æ‰€æœ‰è®¾å¤‡ä¸ŠåŒæ­¥
                    if hasattr(self, 'module') and hasattr(self.module, 'moe_entropy'):
                        # DataParallelç¯å¢ƒï¼ŒåŒæ­¥åˆ°ä¸»æ¨¡å—
                        self.module.moe_entropy = avg_entropy
                    self.moe_entropy = avg_entropy
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡
                    if torch.rand(1).item() < 0.01:  # 1%æ¦‚ç‡æ‰“å°
                        print(f"[è°ƒè¯•] backboneè¿”å›ç†µå€¼: {avg_entropy:.6f}")
                        print(f"[è°ƒè¯•] self.moe_entropyè®¾ç½®: {self.moe_entropy:.6f}")
                else:
                    sat_feat = backbone_output
                    if hasattr(self, 'module') and hasattr(self.module, 'moe_entropy'):
                        self.module.moe_entropy = 0.0
                    self.moe_entropy = 0.0

                heatmap, bbox = self.head(sat_feat)
                return heatmap, bbox
            
            def get_moe_entropy(self):
                """è·å–MoEç†µå€¼"""
                # ä¼˜å…ˆä»backboneè·å–æœ€æ–°çš„ç†µå€¼
                if hasattr(self.backbone, 'get_moe_entropy'):
                    entropy = self.backbone.get_moe_entropy()
                else:
                    # å…œåº•ï¼šä»ä¸»æ¨¡å—è·å–ç†µå€¼
                    if hasattr(self, 'module') and hasattr(self.module, 'moe_entropy'):
                        entropy = self.module.moe_entropy
                    else:
                        entropy = self.moe_entropy
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100æ¬¡è°ƒç”¨æ‰“å°ä¸€æ¬¡
                if torch.rand(1).item() < 0.01:  # 1%æ¦‚ç‡æ‰“å°
                    print(f"[è°ƒè¯•] get_moe_entropyè¿”å›: {entropy:.6f}")
                return entropy
            
            def get_backbone_moe_entropy(self):
                """ä»backboneè·å–MoEç†µå€¼"""
                if hasattr(self.backbone, 'get_moe_entropy'):
                    return self.backbone.get_moe_entropy()
                return 0.0
        
        model = DetGeoSwinMoE_AF(swin_backbone, anchorfree_head)
    else:
        # å¯¹äºéswinmoeæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤çš„DetGeoæ¨¡å‹
        from model.detgeo_swinmoe import DetGeo
        model = DetGeo()
    
    # åŒ…è£…æ¨¡å‹ä»¥æ”¯æŒå¤šGPUè®­ç»ƒ
    model = wrap_model_for_multi_gpu(model, device_ids, args)
    
    # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é¢„è®­ç»ƒæƒé‡
    if args.pretrain:
        model = load_pretrain(model, args, logging)
    elif swin_cfg.get('pretrained') and os.path.exists(swin_cfg['pretrained']):
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é¢„è®­ç»ƒæƒé‡
        args.pretrain = swin_cfg['pretrained']
        model = load_pretrain(model, args, logging)
        logging.info(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é¢„è®­ç»ƒæƒé‡: {swin_cfg['pretrained']}")
    else:
        logging.info("â„¹ï¸ æœªä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # ====== MoEä¸“å®¶æƒé‡åˆå§‹åŒ–ï¼ˆç”¨ä¸»å¹²FFNï¼‰ ======
    if args.model == 'swinmoe':
        from model.swin_moe_geo import initialize_moe_experts_from_ffn
        
        # å¤šå¡ç¯å¢ƒä¸‹æ­£ç¡®è·å–backbone
        if hasattr(model, 'module'):
            # DataParallelç¯å¢ƒ
            backbone = model.module.backbone
            logging.info("ğŸ”§ å¤šå¡ç¯å¢ƒï¼šä»model.moduleè·å–backbone")
        else:
            # å•å¡ç¯å¢ƒ
            backbone = model.backbone
            logging.info("ğŸ”§ å•å¡ç¯å¢ƒï¼šä»modelè·å–backbone")
        
        # ä¿®å¤åçš„MoEæ¶æ„ï¼šæ¯ä¸ªstageæœ‰è‡ªå·±çš„ä¸“å®¶æ± ï¼Œä½†å…±äº«ä¸“å®¶æ•°é‡
        total_experts = 0
        logging.info(f"[DEBUG] backboneç±»å‹: {type(backbone)}")
        logging.info(f"[DEBUG] backbone.stagesæ•°é‡: {len(backbone.stages)}")
        
        for stage_idx, stage in enumerate(backbone.stages):
            logging.info(f"[DEBUG] Stage {stage_idx}: type={type(stage)}, hasattr(expert_pool)={hasattr(stage, 'expert_pool')}")
            if hasattr(stage, 'expert_pool') and stage.expert_pool is not None:
                logging.info(f"[DEBUG] Stage {stage_idx} expert_pool: {stage.expert_pool}")
                # æ‰¾åˆ°åŒstageç¬¬ä¸€ä¸ªéMoE Blockçš„ffnä½œä¸ºå‚è€ƒ
                ffn_ref = None
                for block_idx, block in enumerate(stage.blocks):
                    logging.info(f"[DEBUG] Block {block_idx}: type={type(block)}, hasattr(use_moe)={hasattr(block, 'use_moe')}, use_moe={getattr(block, 'use_moe', False)}")
                    if hasattr(block, 'use_moe') and not block.use_moe:
                        ffn_ref = block.ffn
                        logging.info(f"[DEBUG] æ‰¾åˆ°å‚è€ƒFFN: {ffn_ref}")
                        break
                
                if ffn_ref is not None:
                    # ç”¨å‚è€ƒFFNåˆå§‹åŒ–ä¸“å®¶æ± ä¸­çš„æ‰€æœ‰ä¸“å®¶
                    initialize_moe_experts_from_ffn(stage.expert_pool, ffn_ref)
                    logging.info(f"[MoEä¸“å®¶åˆå§‹åŒ–] Stage {stage_idx} ä¸“å®¶æ± å·²ç”¨æ™®é€šFFNæƒé‡åˆå§‹åŒ–")
                    total_experts += stage.expert_pool.num_experts
                else:
                    logging.info(f"[MoEä¸“å®¶åˆå§‹åŒ–] Stage {stage_idx} æœªæ‰¾åˆ°æ™®é€šFFNï¼Œè·³è¿‡ä¸“å®¶æ± åˆå§‹åŒ–")
            else:
                logging.info(f"[MoEä¸“å®¶åˆå§‹åŒ–] Stage {stage_idx} æ²¡æœ‰ä¸“å®¶æ± ")
        
        # éªŒè¯MoEæ¶æ„ä¿®å¤ç»“æœ
        logging.info(f"[MoEéªŒè¯] æ€»ä¸“å®¶æ•°é‡: {total_experts} (é…ç½®: {swin_cfg.get('num_experts', 6)})")
        if total_experts == swin_cfg.get('num_experts', 6):
            logging.info("âœ… MoEæ¶æ„ä¿®å¤æˆåŠŸï¼ä¸“å®¶æ•°é‡é…ç½®æ­£ç¡®")
        else:
            logging.warning(f"âš ï¸ MoEæ¶æ„ä¸“å®¶æ•°é‡ä¸åŒ¹é…: å®é™…{total_experts} vs é…ç½®{swin_cfg.get('num_experts', 6)}")
    
    # åŸºäºæœ€ä½³æ—¥å¿—çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    warmup_epochs = 2  # å‡å°‘é¢„çƒ­è½®æ¬¡ï¼Œæ›´å¿«è¿›å…¥å­¦ä¹ 
    total_epochs = args.max_epoch
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # ä¼˜åŒ–é¢„çƒ­è°ƒåº¦å™¨ - æ›´å¹³ç¼“çš„é¢„çƒ­
    warmup_scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: min(1.0, (epoch + 1) / warmup_epochs))
    
    # ====== åŠ¨æ€ç†µæ­£åˆ™æƒé‡ ======
    if args.no_moe_entropy:
        def get_lambda_entropy(epoch, max_epoch, base=None, min_val=0.001, warmup=2, best_epoch=None, freeze_after=2):
            return 0.0
    else:
        # ä¿®å¤MoEç†µæ­£åˆ™åŒ–ç­–ç•¥ - ä½¿ç”¨åŸæ¥çš„æƒé‡ï¼Œé—®é¢˜å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹
        base = 0.005  # æ¢å¤åŸæ¥çš„åŸºç¡€æƒé‡
        def get_lambda_entropy(epoch, max_epoch, base=None, min_val=0.001, warmup=2, best_epoch=None, freeze_after=5):
            """
            @function get_lambda_entropy
            @desc åŠ¨æ€è°ƒæ•´MoEç†µæ­£åˆ™åŒ–å¼ºåº¦ï¼Œè®©MoEåœ¨è®­ç»ƒä¸­åæœŸå‘æŒ¥æ ¸å¿ƒä½œç”¨
            @param {int} epoch - å½“å‰è½®æ•°
            @param {int} max_epoch - æ€»è½®æ•°
            @param {float} base - åŸºç¡€ç†µæ­£åˆ™åŒ–å¼ºåº¦
            @param {float} min_val - æœ€å°ç†µæ­£åˆ™åŒ–å¼ºåº¦
            @param {int} warmup - é¢„çƒ­è½®æ•°
            @param {int} best_epoch - æœ€ä½³è½®æ•°ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
            @param {int} freeze_after - å†»ç»“è½®æ•°
            @return {float} å½“å‰è½®æ•°çš„ç†µæ­£åˆ™åŒ–å¼ºåº¦
            """
            if base is None:
                base = 0.005  # æ¢å¤åŸæ¥çš„åŸºç¡€å¼ºåº¦
            if best_epoch is None:
                best_epoch = max_epoch // 2  # é»˜è®¤æœ€ä½³è½®æ•°ä¸ºæ€»è½®æ•°çš„ä¸€åŠ
            
            # é¢„çƒ­é˜¶æ®µï¼šé€æ¸å¢åŠ ç†µæ­£åˆ™åŒ–
            if epoch < warmup:
                lambda_entropy = base * (epoch / warmup)
            # å†»ç»“é˜¶æ®µï¼šä¿æŒç¨³å®šå¼ºåº¦
            elif epoch < freeze_after:
                lambda_entropy = base
            # åŠ¨æ€è°ƒæ•´é˜¶æ®µï¼šæ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´
            else:
                # åœ¨è®­ç»ƒä¸­åæœŸï¼Œé™ä½ç†µæ­£åˆ™åŒ–ï¼Œè®©MoEå‘æŒ¥æ ¸å¿ƒä½œç”¨
                progress = (epoch - freeze_after) / (max_epoch - freeze_after)
                decay_factor = 0.8  # è¡°å‡å› å­ï¼Œè®©MoEåœ¨åæœŸæ›´è‡ªç”±
                lambda_entropy = base * (1 - progress * decay_factor)
            
            # ç¡®ä¿ä¸ä½äºæœ€å°å€¼
            lambda_entropy = max(lambda_entropy, min_val)
            
            return lambda_entropy
    
    # ä¼˜åŒ–å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ - è§£å†³å­¦ä¹ ç‡ä¸‹é™è¿‡å¿«é—®é¢˜
    if args.cosine:
        # å½»åº•ä¿®å¤ä½™å¼¦é€€ç«è°ƒåº¦ - é˜²æ­¢å­¦ä¹ ç‡ä¸‹é™è¿‡å¿«
        cosine_scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs, eta_min=1e-5)
        scheduler = None  # ç¡®ä¿schedulerå˜é‡å­˜åœ¨
    else:
        # åŸºäºæœ€ä½³æ—¥å¿—çš„ReduceLROnPlateauç­–ç•¥ï¼Œæ›´ä¿å®ˆçš„patience
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=5, min_lr=1e-6)
        cosine_scheduler = None  # ç¡®ä¿cosine_schedulerå˜é‡å­˜åœ¨
    
    # è®­ç»ƒå¾ªç¯
    if not args.test and not args.val:
        val_history = []  # è®°å½•æ¯è½®éªŒè¯æŒ‡æ ‡å’Œæƒé‡
        best_val_accu = -float('inf')
        best_epoch = 0
        lambda_entropy = args.lambda_entropy_base
        
        # ç§»é™¤æ—©åœæœºåˆ¶ï¼Œå…ˆè§‚å¯Ÿè®­ç»ƒæ•ˆæœ
        for epoch in range(total_epochs):
            logging.info(f'========== Epoch {epoch+1}/{total_epochs} ==========',)
            if epoch > best_epoch + 2:
                lambda_entropy = 0.0
            else:
                lambda_entropy = get_lambda_entropy(epoch, total_epochs, base=args.lambda_entropy_base)
            train_metrics = train_epoch(train_loader, model, optimizer, epoch, args, trainer, lambda_entropy, device_ids, device)
            # è§£åŒ…è®­ç»ƒæŒ‡æ ‡
            avg_loss, avg_geo_loss, avg_cls_loss, avg_accu50, avg_accu25, avg_mean_iou = train_metrics
            logging.info(f'è®­ç»ƒå®Œæˆ - Loss: {avg_loss:.4f}, Geo Loss: {avg_geo_loss:.4f}, Cls Loss: {avg_cls_loss:.4f}')
            logging.info(f'è®­ç»ƒæŒ‡æ ‡ - Accu50: {avg_accu50:.4f}, Accu25: {avg_accu25:.4f}, Mean IoU: {avg_mean_iou:.4f}')
            logging.info(f'Current Learning Rate: {optimizer.param_groups[0]["lr"]:.2e}')
            
            logging.info(f'\n=== å¼€å§‹éªŒè¯è¯„ä¼° ===')
            val_metrics = test_epoch(val_loader, model, args, device_ids, device)
            val_accu50 = val_metrics['accu50']
            val_accu25 = val_metrics['accu25']
            val_mean_iou = val_metrics['mean_iou']
            logging.info(f'éªŒè¯ç»“æœ - Accu50: {val_accu50:.4f}, Accu25: {val_accu25:.4f}, Mean IoU: {val_mean_iou:.4f}')
            
            # å­¦ä¹ ç‡è°ƒåº¦å™¨stepè°ƒç”¨ - åœ¨éªŒè¯å®Œæˆåè°ƒç”¨
            if args.cosine:
                if epoch < warmup_epochs:
                    warmup_scheduler.step()
                    logging.info(f'Warmup Scheduler Step - Epoch {epoch+1}')
                else:
                    cosine_scheduler.step()
                    logging.info(f'Cosine Scheduler Step - Epoch {epoch+1}')
            else:
                # å¯¹äºReduceLROnPlateauï¼Œä½¿ç”¨éªŒè¯å‡†ç¡®ç‡æ¥step
                old_lr = optimizer.param_groups[0]['lr']
                scheduler.step(val_accu50)  # ä½¿ç”¨éªŒè¯å‡†ç¡®ç‡ä½œä¸ºç›‘æ§æŒ‡æ ‡
                new_lr = optimizer.param_groups[0]['lr']
                if old_lr != new_lr:
                    logging.info(f'Learning Rate Changed: {old_lr:.2e} -> {new_lr:.2e}')
                else:
                    logging.info(f'Learning Rate Unchanged: {new_lr:.2e}')
            
            current_lr = optimizer.param_groups[0]['lr']
            logging.info(f'Current Learning Rate: {current_lr:.2e}')
            
            # åªè®°å½•ï¼Œä¸ä¿å­˜æƒé‡
            if args.cosine:
                scheduler_state = cosine_scheduler.state_dict() if epoch >= warmup_epochs else warmup_scheduler.state_dict()
            else:
                scheduler_state = scheduler.state_dict()
            val_history.append({
                'epoch': epoch + 1,
                'accu50': val_accu50,
                'accu25': val_accu25,
                'mean_iou': val_mean_iou,
                'state_dict': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler_state,
                'swin_cfg': swin_cfg,
            })
            logging.info(f'========== End of Epoch {epoch+1} ==========\n')
            # ç§»é™¤æ—©åœæ£€æŸ¥ï¼Œè®©è®­ç»ƒç»§ç»­è¿è¡Œ
            
            # è®°å½•æœ€ä½³epoch
            if val_accu50 > best_val_accu:
                best_val_accu = val_accu50
                best_epoch = epoch
            # å¢å¼ºè®­ç»ƒç›‘æ§æ—¥å¿—
            logging.info(f"Epoch {epoch+1}: lr={current_lr:.6e}, lambda_entropy={lambda_entropy:.4f}, Accu50={val_accu50:.4f}, MeanIoU={val_mean_iou:.4f}")
            logging.info(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f}, å‡ ä½•æŸå¤±: {avg_geo_loss:.4f}, åˆ†ç±»æŸå¤±: {avg_cls_loss:.4f}")
            logging.info(f"è®­ç»ƒIoU: {avg_mean_iou:.4f}, éªŒè¯IoU: {val_mean_iou:.4f}, å·®è·: {abs(avg_mean_iou-val_mean_iou):.4f}")
            
            # MoEä¸“å®¶æ¿€æ´»ç›‘æ§
            if hasattr(model, 'get_backbone_moe_entropy'):
                moe_entropy = model.get_backbone_moe_entropy()
                logging.info(f"ğŸ¯ MoEä¸“å®¶æ¿€æ´»çŠ¶æ€: ç†µå€¼={moe_entropy:.4f} (ç›®æ ‡èŒƒå›´: 0.6-0.9)")
                if moe_entropy < 0.1:
                    logging.warning("âš ï¸  MoEä¸“å®¶æ¿€æ´»ä¸è¶³ï¼ç†µå€¼è¿‡ä½ï¼Œä¸“å®¶å¯èƒ½æœªè¢«å……åˆ†åˆ©ç”¨")
                elif moe_entropy > 0.9:
                    logging.info("âœ… MoEä¸“å®¶æ¿€æ´»è‰¯å¥½ï¼ç†µå€¼åœ¨ç†æƒ³èŒƒå›´å†…")
                else:
                    logging.info("ğŸ”„ MoEä¸“å®¶æ¿€æ´»æ­£å¸¸ï¼Œç»§ç»­è§‚å¯Ÿ")
        logging.info('\n=== è®­ç»ƒå®Œæˆï¼Œå¼€å§‹åˆ†æ ===')
        # è®­ç»ƒç»“æŸåï¼Œä¿å­˜æœ€ä½³ã€æœ€å·®å’Œæœ€ç»ˆæƒé‡
        best_idx = max(range(len(val_history)), key=lambda i: val_history[i]['accu50'])
        worst_idx = min(range(len(val_history)), key=lambda i: val_history[i]['accu50'])
        final_idx = len(val_history) - 1
        weight_dir = trainer.weight_dir
        # æœ€ç»ˆæƒé‡
        torch.save({
            'epoch': val_history[final_idx]['epoch'],
            'state_dict': val_history[final_idx]['state_dict'],
            'accu': val_history[final_idx]['accu50'],
            'optimizer': val_history[final_idx]['optimizer'],
            'scheduler': val_history[final_idx]['scheduler'],
            'swin_cfg': swin_cfg,
        }, os.path.join(weight_dir, 'final_weights.pth'))
        logging.info(f'âœ“ æœ€ç»ˆæƒé‡å·²ä¿å­˜ (Epoch {val_history[final_idx]["epoch"]}, Accu: {val_history[final_idx]["accu50"]:.4f})')
        # æœ€ä½³æƒé‡
        torch.save({
            'epoch': val_history[best_idx]['epoch'],
            'state_dict': val_history[best_idx]['state_dict'],
            'accu': val_history[best_idx]['accu50'],
            'optimizer': val_history[best_idx]['optimizer'],
            'scheduler': val_history[best_idx]['scheduler'],
            'swin_cfg': swin_cfg,
        }, os.path.join(weight_dir, 'best_weights.pth'))
        logging.info(f'âœ“ æœ€ä½³æƒé‡å·²ä¿å­˜ (Epoch {val_history[best_idx]["epoch"]}, Accu: {val_history[best_idx]["accu50"]:.4f})')
        # æœ€å·®æƒé‡
        torch.save({
            'epoch': val_history[worst_idx]['epoch'],
            'state_dict': val_history[worst_idx]['state_dict'],
            'accu': val_history[worst_idx]['accu50'],
            'optimizer': val_history[worst_idx]['optimizer'],
            'scheduler': val_history[worst_idx]['scheduler'],
            'swin_cfg': swin_cfg,
        }, os.path.join(weight_dir, 'worst_weights.pth'))
        logging.info(f'âš  æœ€å·®æƒé‡å·²ä¿å­˜ (Epoch {val_history[worst_idx]["epoch"]}, Accu: {val_history[worst_idx]["accu50"]:.4f})')
        trainer.best_accu = val_history[best_idx]['accu50']
        trainer.best_epoch = val_history[best_idx]['epoch']
        trainer.worst_accu = val_history[worst_idx]['accu50']
        trainer.worst_epoch = val_history[worst_idx]['epoch']
        trainer.visualize_loss_analysis()
        if args.visualize:
            logging.info('\n=== å¼€å§‹å¯è§†åŒ–æ¨¡å‹è¾“å‡º ===')
            trainer.visualize_model_outputs(model, val_loader)
    
    elif args.visualize:
        # åŠ è½½æœ€ä½³æƒé‡è¿›è¡Œå¯è§†åŒ–
        best_weights_path = os.path.join(trainer.weight_dir, 'best_weights.pth')
        if os.path.exists(best_weights_path):
            checkpoint = torch.load(best_weights_path)
            model.load_state_dict(checkpoint['state_dict'])
            logging.info(f"åŠ è½½æœ€ä½³æƒé‡è¿›è¡Œå¯è§†åŒ–: {best_weights_path}")
            trainer.visualize_model_outputs(model, val_loader)
        else:
            logging.info("æœªæ‰¾åˆ°æœ€ä½³æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œå¯è§†åŒ–")
            trainer.visualize_model_outputs(model, val_loader)

def train_epoch(train_loader, model, optimizer, epoch, args, trainer, lambda_entropy, device_ids, device):
    """
    @function train_epoch
    @desc è®­ç»ƒä¸€ä¸ªepochï¼Œlossåœ¨å¤šå¡æ—¶è‡ªåŠ¨å…¨å±€å¹³å‡ï¼Œä¿è¯ä¸å•å¡ä¸€è‡´ï¼Œè‡ªåŠ¨ç»Ÿè®¡MoEé—¨æ§åˆ†å¸ƒå’Œç†µ
    """
    model.train()
    batch_time = AverageMeter()
    avg_losses = AverageMeter()
    avg_cls_losses = AverageMeter()
    avg_geo_losses = AverageMeter()
    avg_accu = AverageMeter()
    avg_accu25 = AverageMeter()
    avg_iou = AverageMeter()
    moe_entropy_list = []  # è®°å½•æ¯stepæ‰€æœ‰MoEç†µ
    moe_gate_stats = []    # è®°å½•é—¨æ§åˆ†å¸ƒ
    print_freq_entropy = max(1, args.print_freq // 2)
    end = time.time()
    
    for batch_idx, batch in enumerate(train_loader):
        try:
            # æ­£ç¡®è§£åŒ…æ•°æ®ï¼Œä¸custom_collate_fnè¿”å›æ ¼å¼åŒ¹é…
            query_imgs, rs_imgs, ori_gt_bbox, idx, click_xy, ori_hw = batch
            
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if query_imgs is None or rs_imgs is None or ori_gt_bbox is None:
                logging.warning(f"æ‰¹æ¬¡ {batch_idx} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            if query_imgs.shape[0] == 0 or rs_imgs.shape[0] == 0:
                logging.warning(f"æ‰¹æ¬¡ {batch_idx} æ•°æ®å½¢çŠ¶å¼‚å¸¸ï¼Œè·³è¿‡")
                continue
            
            # ç¡®ä¿æ•°æ®æ­£ç¡®ç§»åŠ¨åˆ°è®¾å¤‡
            # ç»Ÿä¸€ä½¿ç”¨ä¸»è®¾å¤‡ï¼Œè®©DataParallelè‡ªåŠ¨åˆ†é…
            query_imgs = query_imgs.to(device)
            rs_imgs = rs_imgs.to(device)
            ori_gt_bbox = ori_gt_bbox.to(device)
            
            # è°ƒè¯•è®¾å¤‡åˆ†é…
            if batch_idx % 100 == 0:
                logging.info(f"[è°ƒè¯•] æ•°æ®è®¾å¤‡: query_imgs={query_imgs.device}, rs_imgs={rs_imgs.device}, model={next(model.parameters()).device}")
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            query_imgs = query_imgs.float()
            rs_imgs = rs_imgs.float()
            ori_gt_bbox = ori_gt_bbox.float()
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            # æ£€æŸ¥CUDAå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
                if memory_allocated > 20:  # å¦‚æœä½¿ç”¨è¶…è¿‡20GB
                    logging.warning(f"CUDAå†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_allocated:.2f}GB, å°è¯•æ¸…ç†")
                    torch.cuda.empty_cache()
                    gc.collect()
            
            ori_gt_bbox = torch.clamp(ori_gt_bbox, min=0, max=args.img_size-1)
            
            if args.model == 'swinmoe':
                # Anchor-Freeæµç¨‹
                # æ¨¡å‹å‰å‘æ¨ç†
                heatmap_pred, bbox_pred = model(query_imgs, rs_imgs)
                B, _, H, W = heatmap_pred.shape
                gt_heatmap, gt_bbox, mask = build_target_anchorfree(ori_gt_bbox, H, W, args.img_size, args.img_size)
                
                # ç¡®ä¿ç›®æ ‡æ•°æ®ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                # ç»Ÿä¸€ä½¿ç”¨ä¸»è®¾å¤‡ï¼Œè®©DataParallelè‡ªåŠ¨åˆ†é…
                gt_heatmap = gt_heatmap.to(device)
                gt_bbox = gt_bbox.to(device)
                mask = mask.to(device)
                
                # åŸºäºå†å²æœ€ä½³ç»“æœä¼˜åŒ–æŸå¤±æƒé‡
                # å†å²æœ€ä½³é…ç½®ï¼šçƒ­åŠ›å›¾æŸå¤±æƒé‡è¾ƒä½ï¼Œå‡ ä½•æŸå¤±æƒé‡è¾ƒé«˜
                heatmap_loss, bbox_loss = anchorfree_loss(heatmap_pred, bbox_pred, gt_heatmap, gt_bbox, mask)
                
                # è·å–MoEç†µå€¼
                moe_entropy = 0.0
                # å¤šGPUç¯å¢ƒä¸‹ï¼Œéœ€è¦æ­£ç¡®å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                if len(device_ids) > 1 and hasattr(model, 'module'):
                    # å¤šGPUç¯å¢ƒï¼Œä»moduleè·å–ç†µå€¼
                    if hasattr(model.module, 'get_moe_entropy'):
                        moe_entropy = model.module.get_moe_entropy()
                    elif hasattr(model.module, 'get_backbone_moe_entropy'):
                        moe_entropy = model.module.get_backbone_moe_entropy()
                else:
                    # å•GPUç¯å¢ƒï¼Œç›´æ¥ä»modelè·å–
                    if hasattr(model, 'get_moe_entropy'):
                        moe_entropy = model.get_moe_entropy()
                    elif hasattr(model, 'get_backbone_moe_entropy'):
                        moe_entropy = model.get_backbone_moe_entropy()
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡ç†µå€¼
                if batch_idx % 100 == 0:
                    logging.info(f"[è°ƒè¯•] MoEç†µå€¼è·å–: {moe_entropy:.6f}")
                
                # è°ƒæ•´æŸå¤±æƒé‡ - æ ¹æ®batch sizeåŠ¨æ€è°ƒæ•´
                # å¤šGPUç¯å¢ƒä¸‹ï¼Œbatch sizeç¿»å€ï¼Œéœ€è¦ç›¸åº”è°ƒæ•´æƒé‡
                if len(device_ids) > 1:
                    # å¤šGPUç¯å¢ƒï¼Œbatch sizeç¿»å€ï¼Œå¢åŠ åˆ†ç±»æŸå¤±æƒé‡
                    total_loss = 0.8 * heatmap_loss + 0.2 * bbox_loss
                else:
                    # å•GPUç¯å¢ƒï¼Œä½¿ç”¨å¹³è¡¡æƒé‡
                    total_loss = 0.7 * heatmap_loss + 0.3 * bbox_loss
                
                # æ·»åŠ MoEç†µæŸå¤±åˆ°æ€»æŸå¤±ä¸­ï¼ˆå®‰å…¨æ·»åŠ ï¼Œä¸å½±å“ä¸»çº¿ä»»åŠ¡ï¼‰
                if lambda_entropy > 0 and moe_entropy > 0:
                    entropy_loss = -lambda_entropy * moe_entropy
                    loss = total_loss + entropy_loss
                    # è®°å½•ç†µæŸå¤±ç”¨äºç›‘æ§
                    entropy_loss_value = entropy_loss.item()
                else:
                    loss = total_loss
                    entropy_loss_value = 0.0
                
                loss_geo = bbox_loss
                loss_cls = heatmap_loss
                
                # ====== æ€§èƒ½æŒ‡æ ‡è®¡ç®— ======
                pred_hm = heatmap_pred.sigmoid()
                pred_centers = pred_hm.view(B, -1).argmax(dim=1)
                pred_y = (pred_centers // W).cpu().numpy()
                pred_x = (pred_centers % W).cpu().numpy()
                gt_centers = gt_heatmap.view(B, -1).argmax(dim=1)
                gt_y = (gt_centers // W).cpu().numpy()
                gt_x = (gt_centers % W).cpu().numpy()
                
                ious = []
                for i in range(B):
                    pred_box = bbox_pred[i, :, pred_y[i], pred_x[i]].detach().cpu().numpy()
                    gt_box = gt_bbox[i, :, gt_y[i], gt_x[i]].detach().cpu().numpy()
                    ious.append(compute_iou(pred_box, gt_box))
                
                ious = np.array(ious)
                accu50 = np.mean(ious > 0.5)
                accu25 = np.mean(ious > 0.25)
                mean_iou = np.mean(ious)
                
                # æ›´æ–°æŒ‡æ ‡ç»Ÿè®¡
                avg_accu.update(accu50, B)
                avg_accu25.update(accu25, B)
                avg_iou.update(mean_iou, B)
                
            else:
                # éswinmoeåˆ†æ”¯
                heatmap_pred, bbox_pred = model(query_imgs, rs_imgs)
                B, _, H, W = heatmap_pred.shape
                gt_heatmap, gt_bbox, mask = build_target_anchorfree(ori_gt_bbox, H, W, args.img_size, args.img_size)
                heatmap_loss, bbox_loss = anchorfree_loss(heatmap_pred, bbox_pred, gt_heatmap, gt_bbox, mask)
                # è°ƒæ•´æŸå¤±æƒé‡ - å¢åŠ åˆ†ç±»æŸå¤±æƒé‡
                total_loss = 0.7 * heatmap_loss + 0.3 * bbox_loss
                beta = getattr(args, 'beta', 1.0)
                loss = total_loss
                loss_geo = bbox_loss
                loss_cls = heatmap_loss
                
                # éMoEæ¨¡å‹ï¼Œç†µæŸå¤±ä¸º0
                entropy_loss_value = 0.0
                
                # æ€§èƒ½æŒ‡æ ‡è®¡ç®—ï¼ˆä¸éMoEåˆ†æ”¯ç›¸åŒï¼‰
                pred_hm = heatmap_pred.sigmoid()
                pred_centers = pred_hm.view(B, -1).argmax(dim=1)
                pred_y = (pred_centers // W).cpu().numpy()
                pred_x = (pred_centers % W).cpu().numpy()
                gt_centers = gt_heatmap.view(B, -1).argmax(dim=1)
                gt_y = (gt_centers // W).cpu().numpy()
                gt_x = (gt_centers % W).cpu().numpy()
                
                ious = []
                for i in range(B):
                    pred_box = bbox_pred[i, :, pred_y[i], pred_x[i]].detach().cpu().numpy()
                    gt_box = gt_bbox[i, :, gt_y[i], gt_x[i]].detach().cpu().numpy()
                    ious.append(compute_iou(pred_box, gt_box))
                
                ious = np.array(ious)
                accu50 = np.mean(ious > 0.5)
                accu25 = np.mean(ious > 0.25)
                mean_iou = np.mean(ious)
                
                # æ›´æ–°æŒ‡æ ‡ç»Ÿè®¡
                avg_accu.update(accu50, B)
                avg_accu25.update(accu25, B)
                avg_iou.update(mean_iou, B)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ·»åŠ æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # æ›´æ–°æŸå¤±ç»Ÿè®¡
            avg_losses.update(loss.item(), query_imgs.shape[0])
            avg_cls_losses.update(loss_cls.item(), query_imgs.shape[0])
            avg_geo_losses.update(loss_geo.item(), query_imgs.shape[0])
            
                        # è®°å½•MoEç†µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if batch_idx % print_freq_entropy == 0:
                # ä»æ¨¡å‹è·å–MoEç†µ
                if len(device_ids) > 1 and hasattr(model, 'module'):
                    # å¤šGPUç¯å¢ƒï¼Œä»moduleè·å–ç†µå€¼
                    if hasattr(model.module, 'get_moe_entropy'):
                        moe_entropy = model.module.get_moe_entropy()
                    elif hasattr(model.module, 'get_backbone_moe_entropy'):
                        moe_entropy = model.module.get_backbone_moe_entropy()
                    else:
                        moe_entropy = 0.0
                else:
                    # å•GPUç¯å¢ƒï¼Œç›´æ¥ä»modelè·å–
                    if hasattr(model, 'get_moe_entropy'):
                        moe_entropy = model.get_moe_entropy()
                    elif hasattr(model, 'get_backbone_moe_entropy'):
                        moe_entropy = model.get_backbone_moe_entropy()
                    else:
                        moe_entropy = 0.0
                # ç¡®ä¿æ˜¯æ ‡é‡å€¼
                if torch.is_tensor(moe_entropy):
                    moe_entropy = moe_entropy.cpu().item()
                moe_entropy_list.append(moe_entropy)
            
            # æ¯æ¬¡éƒ½è¦è·å–å½“å‰MoEç†µç”¨äºæ˜¾ç¤º
            current_moe_entropy = 0.0
            if len(device_ids) > 1 and hasattr(model, 'module'):
                # å¤šGPUç¯å¢ƒï¼Œä»moduleè·å–ç†µå€¼
                if hasattr(model.module, 'get_moe_entropy'):
                    current_moe_entropy = model.module.get_moe_entropy()
                elif hasattr(model.module, 'get_backbone_moe_entropy'):
                    current_moe_entropy = model.module.get_backbone_moe_entropy()
            else:
                # å•GPUç¯å¢ƒï¼Œç›´æ¥ä»modelè·å–
                if hasattr(model, 'get_moe_entropy'):
                    current_moe_entropy = model.get_moe_entropy()
                elif hasattr(model, 'get_backbone_moe_entropy'):
                    current_moe_entropy = model.get_backbone_moe_entropy()
            if torch.is_tensor(current_moe_entropy):
                current_moe_entropy = current_moe_entropy.cpu().item()
            
            batch_time.update(time.time() - end)
            end = time.time()
            
            # æ¯print_freqè¾“å‡ºä¸€æ¬¡
            if (batch_idx + 1) % args.print_freq == 0 or (batch_idx + 1) == len(train_loader):
                logging.info(f"Epoch: [{epoch+1}][{batch_idx+1}/{len(train_loader)}] | "
                              f"Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s) | "
                              f"Loss: {loss.item():.4f} ({avg_losses.avg:.4f}) | "
                              f"Geo Loss: {loss_geo.item():.4f} ({avg_geo_losses.avg:.4f}) | "
                              f"Cls Loss: {loss_cls.item():.4f} ({avg_cls_losses.avg:.4f}) | "
                              f"Accu50: {accu50:.4f} ({avg_accu.avg:.4f}) | "
                              f"Accu25: {accu25:.4f} ({avg_accu25.avg:.4f}) | "
                              f"Mean_IoU: {mean_iou:.4f} ({avg_iou.avg:.4f}) | "
                              f"MoE Entropy: {current_moe_entropy:.4f} | "
                              f"Entropy Loss: {entropy_loss_value:.6f}")
                
                # è¾“å‡ºMoEç†µä¿¡æ¯
                if moe_entropy_list:
                    # ç¡®ä¿tensoråœ¨CPUä¸Šå†è®¡ç®—å¹³å‡å€¼
                    entropy_tensors = [e.cpu().item() if torch.is_tensor(e) else e for e in moe_entropy_list[-print_freq_entropy:]]
                    avg_entropy = np.mean(entropy_tensors)
                    logging.info(f"[MoEé—¨æ§ç†µ] step={batch_idx}, avg_entropy={avg_entropy:.4f}")
            
            # æŸå¤±åˆ†æ
            trainer.analyze_loss_function(loss_cls.item(), loss_geo.item(), loss.item(), accu50, accu25, mean_iou)
            
        except Exception as e:
            import traceback
            logging.error(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            logging.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            continue
    
    return avg_losses.avg, avg_geo_losses.avg, avg_cls_losses.avg, avg_accu.avg, avg_accu25.avg, avg_iou.avg

def test_epoch(data_loader, model, args, device_ids, device):
    """
    éªŒè¯ä¸€ä¸ªepoch
    """
    # å¤šå¡éªŒè¯å½»åº•ä¿®å¤ï¼šä¸´æ—¶è§£é™¤DataParallelåŒ…è£…
    original_model = model
    if len(device_ids) > 1 and hasattr(model, 'module'):
        # å¤šå¡ç¯å¢ƒä¸‹ï¼Œä¸´æ—¶ä½¿ç”¨ä¸»æ¨¡å‹è¿›è¡ŒéªŒè¯
        model = model.module
        logging.info("ğŸ”§ å¤šå¡éªŒè¯ï¼šä¸´æ—¶è§£é™¤DataParallelåŒ…è£…")
    
    model.eval()
    avg_accu50 = AverageMeter()
    avg_accu25 = AverageMeter()
    avg_mean_iou = AverageMeter()
    avg_accu_c = AverageMeter()
    batch_time = AverageMeter()
    
    end = time.time()
    with torch.no_grad():
        for batch_idx, batch in enumerate(data_loader):
            try:
                # æ­£ç¡®è§£åŒ…æ•°æ®ï¼Œä¸custom_collate_fnè¿”å›æ ¼å¼åŒ¹é…
                query_imgs, rs_imgs, ori_gt_bbox, idx, click_xy, ori_hw = batch
                
                # å¤šå¡éªŒè¯ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨ä¸»è®¾å¤‡è¿›è¡ŒéªŒè¯ï¼Œé¿å…DataParallelé—®é¢˜
                if len(device_ids) > 1:
                    # å¤šå¡ç¯å¢ƒä¸‹ï¼Œå¼ºåˆ¶ä½¿ç”¨ä¸»è®¾å¤‡è¿›è¡ŒéªŒè¯ï¼Œé¿å…DataParallelé—®é¢˜
                    query_imgs = query_imgs.to(device)
                    rs_imgs = rs_imgs.to(device)
                    ori_gt_bbox = ori_gt_bbox.to(device)
                else:
                    # å•å¡ç¯å¢ƒæ­£å¸¸å¤„ç†
                    query_imgs = query_imgs.to(device)
                    rs_imgs = rs_imgs.to(device)
                    ori_gt_bbox = ori_gt_bbox.to(device)
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                query_imgs = query_imgs.float()
                rs_imgs = rs_imgs.float()
                ori_gt_bbox = ori_gt_bbox.float()
                
                # å¤šå¡éªŒè¯ä¿®å¤ï¼šæ›´å®‰å…¨çš„å†…å­˜æ¸…ç†
                if len(device_ids) > 1:
                    # å¤šå¡ç¯å¢ƒä¸‹ï¼Œè·³è¿‡å†…å­˜æ¸…ç†ï¼Œé¿å…CUDAé”™è¯¯
                    pass
                else:
                    # å•å¡ç¯å¢ƒæ­£å¸¸æ¸…ç†
                    torch.cuda.empty_cache()
                
                ori_gt_bbox = torch.clamp(ori_gt_bbox, min=0, max=args.img_size-1)
                
                if args.model == 'swinmoe':
                    try:
                        heatmap_pred, bbox_pred = model(query_imgs, rs_imgs)
                        B, _, H, W = heatmap_pred.shape
                        gt_heatmap, gt_bbox, mask = build_target_anchorfree(ori_gt_bbox, H, W, args.img_size, args.img_size)
                        
                        # å¤šå¡éªŒè¯ä¿®å¤ï¼šç¡®ä¿ç›®æ ‡æ•°æ®åœ¨ä¸»è®¾å¤‡ä¸Š
                        if len(device_ids) > 1:
                            # å¤šå¡ç¯å¢ƒä¸‹ï¼Œå¼ºåˆ¶ä½¿ç”¨ä¸»è®¾å¤‡
                            gt_heatmap = gt_heatmap.to(device)
                            gt_bbox = gt_bbox.to(device)
                            mask = mask.to(device)
                        else:
                            # å•å¡ç¯å¢ƒæ­£å¸¸å¤„ç†
                            gt_heatmap = gt_heatmap.to(device)
                            gt_bbox = gt_bbox.to(device)
                            mask = mask.to(device)
                        
                        pred_hm = heatmap_pred.sigmoid()
                        pred_centers = pred_hm.view(B, -1).argmax(dim=1)
                        pred_y = (pred_centers // W).cpu().numpy()
                        pred_x = (pred_centers % W).cpu().numpy()
                        gt_centers = gt_heatmap.view(B, -1).argmax(dim=1)
                        gt_y = (gt_centers // W).cpu().numpy()
                        gt_x = (gt_centers % W).cpu().numpy()
                        
                        ious = []
                        for i in range(B):
                            pred_box = bbox_pred[i, :, pred_y[i], pred_x[i]].detach().cpu().numpy()
                            gt_box = gt_bbox[i, :, gt_y[i], gt_x[i]].detach().cpu().numpy()
                            ious.append(compute_iou(pred_box, gt_box))
                        
                        ious = np.array(ious)
                        accu50 = np.mean(ious > 0.5)
                        accu25 = np.mean(ious > 0.25)
                        mean_iou = np.mean(ious)
                        accu_c = np.mean((pred_x == gt_x) & (pred_y == gt_y))
                        
                        avg_accu50.update(accu50, query_imgs.shape[0])
                        avg_accu25.update(accu25, query_imgs.shape[0])
                        avg_mean_iou.update(mean_iou, query_imgs.shape[0])
                        avg_accu_c.update(accu_c, query_imgs.shape[0])
                        
                    except RuntimeError as e:
                        logging.error(f"æ¨¡å‹æ¨ç†é”™è¯¯: {e}")
                        # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        accu50 = 0.0
                        accu25 = 0.0
                        mean_iou = 0.0
                        accu_c = 0.0
                        avg_accu50.update(accu50, query_imgs.shape[0])
                        avg_accu25.update(accu25, query_imgs.shape[0])
                        avg_mean_iou.update(mean_iou, query_imgs.shape[0])
                        avg_accu_c.update(accu_c, query_imgs.shape[0])
                        
                else:
                    from model.loss import build_target
                    from utils.utils import eval_iou_acc
                    anchors_full = np.array([float(x.strip()) for x in args.anchors.split(',')])
                    anchors_full = anchors_full.reshape(-1, 2)[::-1].copy()
                    anchors_full = torch.tensor(anchors_full, dtype=torch.float32)
                    # å¤šå¡éªŒè¯ä¿®å¤ï¼šç¡®ä¿anchorsåœ¨ä¸»è®¾å¤‡ä¸Š
                    if len(device_ids) > 1:
                        anchors_full = anchors_full.to(device)
                    else:
                        anchors_full = anchors_full.to(device)
                    pred_anchor, attn_score = model(query_imgs, rs_imgs, click_xy)
                    pred_anchor = pred_anchor.view(pred_anchor.shape[0], 9, 5, pred_anchor.shape[2], pred_anchor.shape[3])
                    _, best_anchor_gi_gj = build_target(ori_gt_bbox, anchors_full, args.img_size, pred_anchor.shape[3])
                    accu_list, accu_center, iou, each_acc_list, _, _ = eval_iou_acc(
                        pred_anchor, ori_gt_bbox, anchors_full, best_anchor_gi_gj[:, 1], best_anchor_gi_gj[:, 2],
                        args.img_size, iou_threshold_list=[0.5, 0.25])
                    accu50 = accu_list[0]
                    accu25 = accu_list[1]
                    mean_iou = iou
                    accu_c = accu_center
                    avg_accu50.update(accu50, query_imgs.shape[0])
                    avg_accu25.update(accu25, query_imgs.shape[0])
                    avg_mean_iou.update(mean_iou, query_imgs.shape[0])
                    avg_accu_c.update(accu_c, query_imgs.shape[0])
                
                batch_time.update(time.time() - end)
                end = time.time()
                
                # å¤šå¡éªŒè¯ä¿®å¤ï¼šæ›´å®‰å…¨çš„å†…å­˜æ¸…ç†
                if len(device_ids) <= 1:
                    # å•å¡ç¯å¢ƒæ­£å¸¸æ¸…ç†
                    torch.cuda.empty_cache()
                
                # åªæ¯print_freqè¾“å‡ºä¸€æ¬¡
                if (batch_idx + 1) % args.print_freq == 0 or (batch_idx + 1) == len(data_loader):
                    logging.info(f"[{batch_idx+1}/{len(data_loader)}] | "
                                  f"Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s) | "
                                  f"Accu50: {accu50:.4f} ({avg_accu50.avg:.4f}) | "
                                  f"Accu25: {accu25:.4f} ({avg_accu25.avg:.4f}) | "
                                  f"Mean_IoU: {mean_iou:.4f} ({avg_mean_iou.avg:.4f}) | "
                                  f"Accu_c: {accu_c:.4f} ({avg_accu_c.avg:.4f})")
                                  
            except Exception as e:
                import traceback
                logging.error(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                logging.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                # ä½¿ç”¨é»˜è®¤å€¼ç»§ç»­
                accu50 = 0.0
                accu25 = 0.0
                mean_iou = 0.0
                accu_c = 0.0
                avg_accu50.update(accu50, query_imgs.shape[0])
                avg_accu25.update(accu25, query_imgs.shape[0])
                avg_mean_iou.update(mean_iou, query_imgs.shape[0])
                avg_accu_c.update(accu_c, query_imgs.shape[0])
                continue
    
    # å¤šå¡éªŒè¯ä¿®å¤ï¼šæ¢å¤åŸå§‹æ¨¡å‹
    if len(device_ids) > 1 and original_model != model:
        model = original_model
        logging.info("ğŸ”§ å¤šå¡éªŒè¯ï¼šæ¢å¤DataParallelåŒ…è£…")
    
    # æ±‡æ€»è¾“å‡º
    logging.info("\n=== éªŒè¯é›†æ±‡æ€»ç»“æœ ===")
    logging.info(f"Accu50: {avg_accu50.avg:.4f}, Accu25: {avg_accu25.avg:.4f}, Mean IoU: {avg_mean_iou.avg:.4f}, Accu_c: {avg_accu_c.avg:.4f}")
    return {
        'accu50': avg_accu50.avg,
        'accu25': avg_accu25.avg,
        'mean_iou': avg_mean_iou.avg,
        'accu_c': avg_accu_c.avg
    }

def compute_iou(box1, box2):
    """è®¡ç®—IoU"""
    x1_1 = box1[0] - box1[2] / 2
    y1_1 = box1[1] - box1[3] / 2
    x2_1 = box1[0] + box1[2] / 2
    y2_1 = box1[1] + box1[3] / 2
    
    x1_2 = box2[0] - box2[2] / 2
    y1_2 = box2[1] - box2[3] / 2
    x2_2 = box2[0] + box2[2] / 2
    y2_2 = box2[1] + box2[3] / 2
    
    inter_x1 = max(x1_1, x1_2)
    inter_y1 = max(y1_1, y1_2)
    inter_x2 = min(x2_1, x2_2)
    inter_y2 = min(y2_1, y2_2)
    
    inter_w = max(0, inter_x2 - inter_x1)
    inter_h = max(0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = area1 + area2 - inter_area
    
    if union_area == 0:
        return 0.0
    return inter_area / union_area

def _extract_meter_values(meter_list):
    """
    å°†AverageMeterå¯¹è±¡åˆ—è¡¨è½¬ä¸ºfloatæ•°å€¼åˆ—è¡¨
    """
    return [x.avg if hasattr(x, 'avg') else float(x) for x in meter_list]

if __name__ == '__main__':
    main()